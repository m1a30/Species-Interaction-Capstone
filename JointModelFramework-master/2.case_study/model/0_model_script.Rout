
R version 3.6.3 (2020-02-29) -- "Holding the Windsock"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> # Objective: running a joint model combining NDDM and RIM to estimate all interactions at the same time. 
> # 
> # Run in Terminal: 
> #   
> #   R CMD BATCH '--args 0' model.R model/0_model_script.Rout
> # 
> # substituting '0' for the comm number (0, 1, 2, or 3) 
> # 
> # This calls up model.R, which runs joint_model.stan. 
> # results got to model/output, /transformed and /validation
> 
> 
> # Run the joint model: 
> # 1. Estimate inferrable interactions with NDDM 
> # 2. Estimate response and effect interactions when non-inferrable
> 
> # Then: 
> # 3. Extract the intrinsic growth rates 
> # 5. Scale the interactions into per-capita growth rates
> 
> # PRELUDE
> #-------
> Sys.time()
[1] "2022-02-09 11:30:24 AEST"
> 
> # Get arguments from bash script
> #!/usr/bin/env Rscript
> args = commandArgs(trailingOnly=TRUE)
> # take comm name as an argument from bash
> if (length(args)==0) {
+   stop("At least one argument must be supplied (input file).n", call.=FALSE)
+ } 
> if (length(args)>1) {
+   stop("Model can only be run on 1 comm at a time.n", call.=FALSE)
+ }
> comm <- args[1]
> 
> # set up R environment
> library(rstan)
Loading required package: StanHeaders
Loading required package: ggplot2
rstan (Version 2.21.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)
> rstan_options(auto_write = TRUE)
> options(mc.cores = parallel::detectCores()) 
> 
> library(rethinking)
Loading required package: parallel
rethinking (Version 1.59)
> library(reshape2)
> library(here)
here() starts at /home/uqmbimle/Dropbox/Work/Projects/2020_Methods_for_compnet
> 
> setwd(here('2.case_study/'))
> 
> source('../1.code/data_prep.R')
> source('functions/stan_modelcheck_rem.R')
> source('functions/scale_interactions.R')
> 
> # Load, verify & prepare data
> #----------------------------
> fecundities <- read.csv(paste0('data/fecundities', comm, '.csv'), stringsAsFactors = F)
> 
> # Keep note of how focals and neighbours are indexed
> key_speciesID <- unlist(read.csv(paste0('data/key_speciesID', comm, '.csv'), stringsAsFactors = F))
> key_neighbourID <- unlist(read.csv(paste0('data/key_neighbourID', comm, '.csv'), stringsAsFactors = F))
> 
> # ensure neighbours are linearly independent across the whole dataset
> N_all <- apply(fecundities[ , 5:dim(fecundities)[2]], c(1,2), as.numeric)
> X_all <- cbind(model.matrix(~as.factor(fecundities$focal)), N_all)
> R_all <- pracma::rref(X_all)
> Z_all <- t(R_all) %*% R_all
> indep <- sapply(seq(1, dim(Z_all)[1], 1), function(k){ 
+   ifelse(Z_all[k, k] == 1 & sum(Z_all[k, -k]) == 0, 1, 0)
+ }) #
> all(indep == 1) # if TRUE then neighbours are linearly independent and we can continue
[1] TRUE
> if(!all(indep == 1)) message('WARNING neighbours are not linearly independent') 
> 
> # transform data into format required by STAN
> stan.data <- data_prep(perform = 'seeds', focal = 'focal', 
+                        nonNcols = 4, df = fecundities)
> 
> 
> message(paste0('Community selected: ', comm))
Community selected: 0
> message(paste0('Fecundity data dimensions = ', dim(fecundities)[1], ', ', dim(fecundities)[2]))
Fecundity data dimensions = 5736, 56
> message(paste0('Number of focals = ', length(key_speciesID)))
Number of focals = 22
> message(paste0('Number of neighbours = ', length(key_neighbourID)))
Number of neighbours = 52
> message(paste0('Proportion of inferrable interactions = ', sum(stan.data$Q)/(stan.data$S*stan.data$K)))
Proportion of inferrable interactions = 0.567307692307692
> 
> #--------------------------------------------------
> # Estimate interactions with a joint NDD*RI model |
> #--------------------------------------------------
> 
> fit <- stan(file = '../1.code/joint_model.stan',
+             data =  stan.data,               # named list of data
+             chains = 1,
+             warmup = 5000,          # number of warmup iterations per chain
+             iter = 10000,            # total number of iterations per chain
+             refresh = 100,         # show progress every 'refresh' iterations
+             control = list(max_treedepth = 10)
+ )
hash mismatch so recompiling; make sure Stan code ends with a blank line

SAMPLING FOR MODEL 'joint_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.014784 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 147.84 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 10000 [  0%]  (Warmup)
Chain 1: Iteration:  100 / 10000 [  1%]  (Warmup)
Chain 1: Iteration:  200 / 10000 [  2%]  (Warmup)
Chain 1: Iteration:  300 / 10000 [  3%]  (Warmup)
Chain 1: Iteration:  400 / 10000 [  4%]  (Warmup)
Chain 1: Iteration:  500 / 10000 [  5%]  (Warmup)
Chain 1: Iteration:  600 / 10000 [  6%]  (Warmup)
Chain 1: Iteration:  700 / 10000 [  7%]  (Warmup)
Chain 1: Iteration:  800 / 10000 [  8%]  (Warmup)
Chain 1: Iteration:  900 / 10000 [  9%]  (Warmup)
Chain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)
Chain 1: Iteration: 1100 / 10000 [ 11%]  (Warmup)
Chain 1: Iteration: 1200 / 10000 [ 12%]  (Warmup)
Chain 1: Iteration: 1300 / 10000 [ 13%]  (Warmup)
Chain 1: Iteration: 1400 / 10000 [ 14%]  (Warmup)
Chain 1: Iteration: 1500 / 10000 [ 15%]  (Warmup)
Chain 1: Iteration: 1600 / 10000 [ 16%]  (Warmup)
Chain 1: Iteration: 1700 / 10000 [ 17%]  (Warmup)
Chain 1: Iteration: 1800 / 10000 [ 18%]  (Warmup)
Chain 1: Iteration: 1900 / 10000 [ 19%]  (Warmup)
Chain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)
Chain 1: Iteration: 2100 / 10000 [ 21%]  (Warmup)
Chain 1: Iteration: 2200 / 10000 [ 22%]  (Warmup)
Chain 1: Iteration: 2300 / 10000 [ 23%]  (Warmup)
Chain 1: Iteration: 2400 / 10000 [ 24%]  (Warmup)
Chain 1: Iteration: 2500 / 10000 [ 25%]  (Warmup)
Chain 1: Iteration: 2600 / 10000 [ 26%]  (Warmup)
Chain 1: Iteration: 2700 / 10000 [ 27%]  (Warmup)
Chain 1: Iteration: 2800 / 10000 [ 28%]  (Warmup)
Chain 1: Iteration: 2900 / 10000 [ 29%]  (Warmup)
Chain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)
Chain 1: Iteration: 3100 / 10000 [ 31%]  (Warmup)
Chain 1: Iteration: 3200 / 10000 [ 32%]  (Warmup)
Chain 1: Iteration: 3300 / 10000 [ 33%]  (Warmup)
Chain 1: Iteration: 3400 / 10000 [ 34%]  (Warmup)
Chain 1: Iteration: 3500 / 10000 [ 35%]  (Warmup)
Chain 1: Iteration: 3600 / 10000 [ 36%]  (Warmup)
Chain 1: Iteration: 3700 / 10000 [ 37%]  (Warmup)
Chain 1: Iteration: 3800 / 10000 [ 38%]  (Warmup)
Chain 1: Iteration: 3900 / 10000 [ 39%]  (Warmup)
Chain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)
Chain 1: Iteration: 4100 / 10000 [ 41%]  (Warmup)
Chain 1: Iteration: 4200 / 10000 [ 42%]  (Warmup)
Chain 1: Iteration: 4300 / 10000 [ 43%]  (Warmup)
Chain 1: Iteration: 4400 / 10000 [ 44%]  (Warmup)
Chain 1: Iteration: 4500 / 10000 [ 45%]  (Warmup)
Chain 1: Iteration: 4600 / 10000 [ 46%]  (Warmup)
Chain 1: Iteration: 4700 / 10000 [ 47%]  (Warmup)
Chain 1: Iteration: 4800 / 10000 [ 48%]  (Warmup)
Chain 1: Iteration: 4900 / 10000 [ 49%]  (Warmup)
Chain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)
Chain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)
Chain 1: Iteration: 5100 / 10000 [ 51%]  (Sampling)
Chain 1: Iteration: 5200 / 10000 [ 52%]  (Sampling)
Chain 1: Iteration: 5300 / 10000 [ 53%]  (Sampling)
Chain 1: Iteration: 5400 / 10000 [ 54%]  (Sampling)
Chain 1: Iteration: 5500 / 10000 [ 55%]  (Sampling)
Chain 1: Iteration: 5600 / 10000 [ 56%]  (Sampling)
Chain 1: Iteration: 5700 / 10000 [ 57%]  (Sampling)
Chain 1: Iteration: 5800 / 10000 [ 58%]  (Sampling)
Chain 1: Iteration: 5900 / 10000 [ 59%]  (Sampling)
Chain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)
Chain 1: Iteration: 6100 / 10000 [ 61%]  (Sampling)
Chain 1: Iteration: 6200 / 10000 [ 62%]  (Sampling)
Chain 1: Iteration: 6300 / 10000 [ 63%]  (Sampling)
Chain 1: Iteration: 6400 / 10000 [ 64%]  (Sampling)
Chain 1: Iteration: 6500 / 10000 [ 65%]  (Sampling)
Chain 1: Iteration: 6600 / 10000 [ 66%]  (Sampling)
Chain 1: Iteration: 6700 / 10000 [ 67%]  (Sampling)
Chain 1: Iteration: 6800 / 10000 [ 68%]  (Sampling)
Chain 1: Iteration: 6900 / 10000 [ 69%]  (Sampling)
Chain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)
Chain 1: Iteration: 7100 / 10000 [ 71%]  (Sampling)
Chain 1: Iteration: 7200 / 10000 [ 72%]  (Sampling)
Chain 1: Iteration: 7300 / 10000 [ 73%]  (Sampling)
Chain 1: Iteration: 7400 / 10000 [ 74%]  (Sampling)
Chain 1: Iteration: 7500 / 10000 [ 75%]  (Sampling)
Chain 1: Iteration: 7600 / 10000 [ 76%]  (Sampling)
Chain 1: Iteration: 7700 / 10000 [ 77%]  (Sampling)
Chain 1: Iteration: 7800 / 10000 [ 78%]  (Sampling)
Chain 1: Iteration: 7900 / 10000 [ 79%]  (Sampling)
Chain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)
Chain 1: Iteration: 8100 / 10000 [ 81%]  (Sampling)
Chain 1: Iteration: 8200 / 10000 [ 82%]  (Sampling)
Chain 1: Iteration: 8300 / 10000 [ 83%]  (Sampling)
Chain 1: Iteration: 8400 / 10000 [ 84%]  (Sampling)
Chain 1: Iteration: 8500 / 10000 [ 85%]  (Sampling)
Chain 1: Iteration: 8600 / 10000 [ 86%]  (Sampling)
Chain 1: Iteration: 8700 / 10000 [ 87%]  (Sampling)
Chain 1: Iteration: 8800 / 10000 [ 88%]  (Sampling)
Chain 1: Iteration: 8900 / 10000 [ 89%]  (Sampling)
Chain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)
Chain 1: Iteration: 9100 / 10000 [ 91%]  (Sampling)
Chain 1: Iteration: 9200 / 10000 [ 92%]  (Sampling)
Chain 1: Iteration: 9300 / 10000 [ 93%]  (Sampling)
Chain 1: Iteration: 9400 / 10000 [ 94%]  (Sampling)
Chain 1: Iteration: 9500 / 10000 [ 95%]  (Sampling)
Chain 1: Iteration: 9600 / 10000 [ 96%]  (Sampling)
Chain 1: Iteration: 9700 / 10000 [ 97%]  (Sampling)
Chain 1: Iteration: 9800 / 10000 [ 98%]  (Sampling)
Chain 1: Iteration: 9900 / 10000 [ 99%]  (Sampling)
Chain 1: Iteration: 10000 / 10000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 2384.45 seconds (Warm-up)
Chain 1:                2771.41 seconds (Sampling)
Chain 1:                5155.86 seconds (Total)
Chain 1: 
> 
> # parameters of interest
> param.vec <- fit@model_pars[!fit@model_pars %in% c('response1', 'responseSm1', 'lp__')]
> 
> 
> # Raw output
> #------------
> save(fit, file = paste0('model/output/model_fit.Rdata')) # model fit
> # Save the 'raw' draws from the posterior
> post.draws <- extract.samples(fit)
> save(post.draws, file = paste0('model/output/post_draws.Rdata'))
> 
> # Save mean, 10% and 90% quantiles for each parameter, as well as n_eff and Rhat
> fit_sum <- summary(fit, pars = param.vec, probs = c(0.1, 0.9))$summary
> write.csv(fit_sum, file = paste0('model/output/summary_of_draws.csv'), row.names = T)
> 
> # Save the logarithm of the (unnormalized) posterior density (lp__)
> log_post <- unlist(extract(fit, 'lp__'))
> write.csv(log_post, file = paste0('model/output/log_post.csv'), row.names = F)
> 
> # Validation
> #------------
> # Get Geweke statistics
> matrix_of_draws <- as.matrix(fit)
> gew <- coda::geweke.diag(matrix_of_draws)
> write.csv(gew$z, 'model/validation/gew_stats.csv')
> # get adjusted values (see boral() package)
> gew.pvals <- 2*pnorm(abs(unlist(gew$z)), lower.tail = FALSE)
> adj.gew <- p.adjust(gew.pvals, method = "holm")
> write.csv(adj.gew, 'model/validation/gew_stats_holmadjust.csv')
> print(paste0('Range of p-values for chain convergence: ', min(na.omit(adj.gew)), ' to ',  max(na.omit(adj.gew))))
[1] "Range of p-values for chain convergence: 0.0239374604665672 to 1"
> 
> png('model/validation/geweke_dist.png', width = 500, height = 500)
> plot(density(na.omit(gew$z)))
> lines(density(rnorm(10000)), col = 'red')
> abline(v = -2, lty = 2)
> abline(v = 2, lty = 2)
> dev.off()
null device 
          1 
> 
> 
> # Diagnostics
> stan_diagnostic(fit, 'model/validation/')

Divergences:
0 of 5000 iterations ended with a divergence.

Tree depth:
0 of 5000 iterations saturated the maximum tree depth of 10.

Energy:
E-BFMI indicated no pathological behavior.
0 of 5000 iterations saturated the maximum tree depth of 10.
E-BFMI indicated no pathological behavior.
0 of 5000 iterations ended with a divergence.
[1] "rhat range: "     "0.99979997999647" "1.00298231436137"
[1] "n_eff range: "    "593.93173555197"  "14789.2038234831"
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
null device 
          1 
> # Traceplots and posterior uncertainty intervals
> stan_model_check(fit, 'model/validation/', params = param.vec)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
   beta_i0.null device   disp_dev.null device    beta_ij.null device 
                     1                      1                      1 
    effect.null device   response.null device  ri_betaij.null device 
                     1                      1                      1 
ndd_betaij.null device 
                     1 
> # Posterior predictive check
> stan_post_pred_check(post.draws, 'mu', 'model/validation/', stan.data)
NULL
null device 
          1 
> stan_post_pred_check(post.draws, 'mu2', 'model/validation/', stan.data)
NULL
null device 
          1 
> 
> # Parameter outputs - draw 1000 samples from the 80% confidence intervals and save 
> #------------------
> # this works for parameters that are not the interaction matrices
> sapply(param.vec[!param.vec %in% c('ri_betaij', 'ndd_betaij')], function(p) {
+   
+   p.samples <- apply(post.draws[[p]], 2, function(x){
+     sample(x[x > quantile(x, 0.1) & x < quantile(x, 0.9)], size = 1000)
+   })
+   write.csv(p.samples, paste0('model/output/', p, '_samples.csv'), 
+             row.names = F)
+   
+ })
$beta_i0
NULL

$disp_dev
NULL

$beta_ij
NULL

$effect
NULL

$response
NULL

$mu
NULL

$mu2
NULL

$log_lik_rim
NULL

$log_lik_nddm
NULL

> 
> # Interactions (betaij)
> # joint interactions 
> betaij <- apply(post.draws$ndd_betaij, c(2, 3), function(x) {
+   sample(x[x > quantile(x, 0.1) & x < quantile(x, 0.9)], size = 1000)
+ })
> betaijS <- as.data.frame(aperm(betaij, perm = c(1, 3, 2)))
> colnames(betaijS) <- grep('ndd_betaij', rownames(fit_sum), value = T)
> write.csv(betaijS, paste0('model/output/joint_betaij_samples.csv'), row.names = F)
> 
> # rim interactions only
> rim_betaij <- apply(post.draws$ri_betaij, c(2, 3), function(x) {
+   sample(x[x > quantile(x, 0.1) & x < quantile(x, 0.9)], size = 1000)
+ })
> rim_betaijS <- as.data.frame(aperm(rim_betaij, perm = c(1, 3, 2)))
> colnames(rim_betaijS) <- grep('ri_betaij', rownames(fit_sum), value = T)
> write.csv(rim_betaijS, paste0('model/output/RIM_betaij_samples.csv'), row.names = F)
> 
> # replace uninferrable interactions with 0 to get the NDDM estimates only 
> Q <- as.vector(t(stan.data$Q)) # the t() is very important to fill Q by row! 
> nddm_betaij <- sweep(betaijS, 2, Q, `*`)
> write.csv(nddm_betaij, paste0('model/output/NDDM_betaij_samples.csv'), row.names = F)
> 
> # Let's do a few plots while we're at it
> # interactions inferred by the NDDM only
> nddm_betaij_inf <-as.matrix(nddm_betaij[  , which(colSums(nddm_betaij) != 0)]) # NDDM without non-inferrables
> # RIM estimates for those inferrable interactions
> rim_betaij_inf <- as.matrix(rim_betaijS[  , which(colSums(nddm_betaij) != 0)]) # NDDM without non-inferrables
> # RIM estimates for non-inferrable interactions only
> rim_betaij_noinf <- as.matrix(rim_betaijS[  , which(colSums(nddm_betaij) == 0)])
> 
> # Check estimates of inferrable interactions from both models 
> png(paste0('model/validation/nddm_vs_rim_alphas.png'))
> plot(nddm_betaij_inf, rim_betaij_inf, 
+      xlab = 'NDDM interactions (inferrable only)', 
+      ylab = 'RIM interactions (inferrable only)',
+      xlim = c(min(nddm_betaij), max(nddm_betaij)),
+      ylim = c(min(nddm_betaij), max(nddm_betaij)))
> abline(0,1)
> dev.off()
null device 
          1 
> 
> # check distribution of inferrable and non-inferrable interactions
> png(paste0('model/validation/betaij_est_distr.png'))
> par(mfrow=c(3,1))
> hist(nddm_betaij_inf, xlab = "", breaks = 30,
+      main = "Inferrable interactions (NDDM)", xlim = c(min(betaijS), max(betaijS)))
> hist(rim_betaij_inf,  xlab = "", breaks = 30,
+      main = 'Inferrable interactions (RIM)', xlim = c(min(betaijS), max(betaijS)))
> hist(rim_betaij_noinf,  xlab = "", breaks = 30,
+      main = 'Non-inferrable interactions (RIM)', xlim = c(min(betaijS), max(betaijS)))
> dev.off()
null device 
          1 
> 
> # Transformed parameters
> #-----------------------
> # Intrinsic growth rate (lambda)
> growth.rates.samples <- apply(post.draws$beta_i0, 2, function(x){
+   sample(x[x > quantile(x, 0.1) & x < quantile(x, 0.9)], size = 1000)
+ })
> # exponentiate to get lambda
> growth.rates.samples <- exp(growth.rates.samples)
> write.csv(growth.rates.samples, paste0('model/transformed/lambda_samples.csv'), row.names = F)
> 
> # Scale the alphas and save
> scaled_betas <- scale_interactions(betaijS, growth.rates.samples, key_speciesID, key_neighbourID, comm)
Growth rates for community 0
      POLE       POCA       PEAI       WAAC       HYPO       GOPU       PEDU 
 5.3227248  4.6026826  4.2528640  4.1385363  3.8309363  3.3151453  2.8453049 
      STPA       ARCA       PLDE       GOBE       HYGL       GITE       VECY 
 2.7455934  2.6548025  2.5757523  2.5498072  2.4388030  2.4142741  2.3568714 
      VERO       CAHI       PTGA       EROD       MEDI       HAOD       TRCY 
 2.3013481  1.7363732  1.6188857  1.5363879  1.5279258  1.5266977  0.2248293 
      TROR 
-0.7671882 
Growth rates for TRCY replaced by HAOD
Growth rates for TROR replaced by HAOD

> save(scaled_betas, file = paste0('model/transformed/scaled_betaij_matrices.Rdata')) 
> 
> Sys.time()
[1] "2022-02-09 13:21:53 AEST"
> 
> 
> 
> 
> 
> proc.time()
    user   system  elapsed 
6971.321  571.143 6688.346 
